Y=base_rf["GLOBAL_transformados"]
X=base_rf.drop(["GLOBAL_transformados"], axis=1) ## Todo lo dem√°s
X_train, X_test, y_train, y_test=train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1234) ## Garantizando la misma proporcion
## En el train y en el test
pd.DataFrame(y_train).groupby("GLOBAL_transformados").size()/sum(pd.DataFrame(y_train).groupby("GLOBAL_transformados").size())
RF= RandomForestClassifier(min_samples_leaf=20, n_estimators=100, max_features="auto")### 
RF.fit(X_train, y_train)
y_pred_train=RF.predict(X_train)
y_pred_test=RF.predict(X_test)
print("En los datos de entrenamiento \n",classification_report(y_train, y_pred_train))
print("En los datos de prueba \n",classification_report(y_test, y_pred_test))
confusion_matrix(y_test, y_pred_test)
from collections import OrderedDict
RANDOM_STATE=20
ensemble_RF= [
    ("RandomForestClassifier, max_features='sqrt'",
        RandomForestClassifier(warm_start=True,max_depth=10,  oob_score=True,
                               max_features="sqrt",
                               random_state=RANDOM_STATE, max_samples=10000)),
    ("RandomForestClassifier, max_features='log2'",
        RandomForestClassifier(warm_start=True,max_depth=10, max_features='log2',
                               oob_score=True,
                               random_state=RANDOM_STATE, max_samples=10000)),
    ("RandomForestClassifier, max_features=None",
        RandomForestClassifier(warm_start=True,max_depth=10,  max_features=None,
                               oob_score=True,
                               random_state=RANDOM_STATE, max_samples=10000))]

# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
error_rate = OrderedDict((label, []) for label, _ in ensemble_RF)

# Range of `n_estimators` values to explore.
min_estimators = 10
max_estimators = 100

for label, clf in ensemble_RF:
    for i in range(min_estimators, max_estimators + 1):
        clf.set_params(n_estimators=i)
        clf.fit(X_train, y_train)

        # Record the OOB error for each `n_estimators=i` setting.
        oob_error = 1 - clf.oob_score_
        error_rate[label].append((i, oob_error))

# Generate the "OOB error rate" vs. "n_estimators" plot.
for label, clf_err in error_rate.items():
    xs, ys = zip(*clf_err)
    plt.plot(xs, ys, label=label)

plt.xlim(min_estimators, max_estimators)
plt.xlabel("n_estimators")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()
