Y=base_rf["GLOBAL_transformados"]
X=base_rf.drop(["GLOBAL_transformados"], axis=1) ## Todo lo demás
X_train, X_test, y_train, y_test=train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=1234) ## Garantizando la misma proporcion
## En el train y en el test
pd.DataFrame(y_train).groupby("GLOBAL_transformados").size()/sum(pd.DataFrame(y_train).groupby("GLOBAL_transformados").size())
RF= RandomForestClassifier(min_samples_leaf=20, n_estimators=100, max_features="auto")### 
RF.fit(X_train, y_train)
y_pred_train=RF.predict(X_train)
y_pred_test=RF.predict(X_test)
print("En los datos de entrenamiento \n",classification_report(y_train, y_pred_train))
print("En los datos de prueba \n",classification_report(y_test, y_pred_test))
confusion_matrix(y_test, y_pred_test)
from collections import OrderedDict
RANDOM_STATE=20
ensemble_RF= [
    ("RandomForestClassifier, max_features='sqrt'",
        RandomForestClassifier(warm_start=True,max_depth=20,  oob_score=True,
                               max_features="sqrt",
                               random_state=RANDOM_STATE, max_samples=10000)),
    ("RandomForestClassifier, max_features='log2'",
        RandomForestClassifier(warm_start=True,max_depth=20, max_features='log2',
                               oob_score=True,
                               random_state=RANDOM_STATE, max_samples=10000)),
    ("RandomForestClassifier, max_features=None",
        RandomForestClassifier(warm_start=True,max_depth=20,  max_features=None,
                               oob_score=True,
                               random_state=RANDOM_STATE, max_samples=10000))]

# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
error_rate = OrderedDict((label, []) for label, _ in ensemble_RF)

# Range of `n_estimators` values to explore.
min_estimators = 10
max_estimators = 100

for label, clf in ensemble_RF:
    for i in range(min_estimators, max_estimators + 1):
        clf.set_params(n_estimators=i)
        clf.fit(X_train, y_train)

        # Record the OOB error for each `n_estimators=i` setting.
        oob_error = 1 - clf.oob_score_
        error_rate[label].append((i, oob_error))

# Generate the "OOB error rate" vs. "n_estimators" plot.
for label, clf_err in error_rate.items():
    xs, ys = zip(*clf_err)
    plt.plot(xs, ys, label=label)

plt.xlim(min_estimators, max_estimators)
plt.xlabel("n_estimators")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()

# Gráfico de cinco (5) variables más importantes
importancias = RF.feature_importances_
std = np.std([tree.feature_importances_ for tree in RF.estimators_],
             axis=0)
indices = np.argsort(importancias)[::-1]

# Print the feature ranking
print("Ranking de Variables:")

for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % ( f + 1, indices[f], importancias[indices[f]]))

plt.figure()
plt.title("Ranking de Variables")
plt.bar(range(X.shape[1]), importancias[indices],color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

importancias
X_train

importance = pd.DataFrame(importancias, columns=["importancia"], index=X_train.columns)
importance_df=importance.sort_values(by="importancia",ascending=False)
importance_df=importance_df.head(5)
importance_df.columns
df = importance_df.rename_axis('index').reset_index()
df = df.rename(columns={'index':'variables'})
df


sns.set_theme(style="darkgrid")
plt.figure()
plt.title("Ranking de Variables")
plt.ylabel("Importancia")
plt.grid
plt.xticks(rotation = 90) 
plt.bar(df.variables, df.importancia,color="r", align="center")
